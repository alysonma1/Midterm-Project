---
title: "Midterm Project"
author: "Alyson Longworth"
output: pdf_document
date: "2024-03-23"
---

# **Introduction**

The growing use and occurrence of artificial intelligence in our daily lives pose significant issues that demand understanding since they may have far-reaching consequences for our society. My focus will be on how artificial intelligence can be used in the criminal justice system to enhance predictive policing and risk assessment. Artificial intelligence may improve police effectiveness when used in risk assessment and predictive policing, but some drawbacks must be considered. Its inevitable errors and prejudices may sway our decisions and raise concerns about equity and discrimination. Because the improper use of these algorithms can have detrimental impacts on people's lives and communities, we must continue to be aware of, look into, and critique these tactics.

# **Summary of Method: Predictive Policing and Results**

To distribute resources as effectively as possible to achieve optimum policing efficiency, statistical modeling techniques are often implemented to anticipate the location and timing of future crimes in the criminal justice system. Statistical models are widely practiced in predictive policing to forecast crime trends, including when and where crimes are likely to occur. These models, which look for causal relationships, include point process models (Poisson processes), Euclidean distance, and Bayesian techniques. The most simplistic method of predictive policing relies on the use of time and location and is forecasted using Euclidean distance. While being relatively easy to compute with more readily available data, this method has drawbacks. Although most frequently used in high-crime regions and periods when it is advantageous to achieve speedy results, this approach tends to group individuals. This strategy frequently encourages racial and socioeconomic prejudices since it uses controversial and inevitable proxies, such as zip code. On the other hand, based on past crime trends, Bayesian approaches produce a probability distribution regarding the place and timing of crimes. Point process models are useful because of the commonly available variables of space and time in crime data. Assuming that the crimes follow a Poisson distribution, this model will provide the occurrence rate of crimes at a specific location and time. However, it's also important to acknowledge the drawbacks of this model's conclusions. Although this may be an improvement over the possible biases from the prior techniques, this model assumes a Poisson distribution, which implies total independence across data. Nevertheless, it can occasionally miss important correlations, including location and time. Moreover, Poisson distribution often fails to acknowledge the usual fluctuations in criminal behavior and crime trends. Support vector machines, on the other hand, are popular because they don't need much human intervention, therefore they can potentially prevent bias resulting from human prejudice. These algorithms attempt to identify the hyperplane that best separates the data points after analyzing a dataset, categorizing the points into groups that will aid in decision-making.

# **Summary of Method: Risk Assessment and Results**

Predicting a person’s inclination for criminal activity in the future is known as risk assessment, and it plays a part in decisions like parole eligibility, sentencing, and pretrial release. The primary objective here is to properly portray the characteristics of individuals linked to criminal activity. Simple two-variable associations have given way to more sophisticated statistical techniques like machine learning and logistic regression in this process. Using predictor variables like demographics and past criminal history, logistic regression models calculate the log odds of a binary outcome, a typical example being whether someone will commit a crime or not. However, because this translation can be excessively complex, these results are often misconstructed when they need to be converted into probabilities for risk assessment. However, machine learning techniques like support vector machines and random forests help lessen this problem. Several decision trees that have been trained on a random dataset are combined to create predictions via random forests. These models are helpful for larger datasets and may produce more accurate predictions due to their potential nonlinear relationship between predictor variables and outcomes. Additionally, support vector machines are used in risk assessment to categorize people based on their traits rather than location and time. Support vector machines, however, commonly run the danger of overfitting a random population of data points by being overly particular with only a specific group. 

# **Description of Normative Concern**

Inaccurate predictive results can be caused by biases in training data and model assumptions, and in turn, can create a positive feedback loop toward more biases. Regarding the application of risk assessment and predictive policing, the main normative concern is the role potential biases and inaccuracies play in the likelihood of promoting systematic discrimination against particular groups. Algorithmic predictions may be impacted by biased data, which may ultimately result in the over-policing of certain areas and the promotion of racial and economic prejudice. 

# **References**

Berk, Richard A. “Artificial Intelligence, Predictive Policing, and Risk Assessment for Law Enforcement.” Annual Review of Criminology, vol. 4, no. 1, Jan. 2021, pp. 209–237, https://doi.org/https://doi.org/10.1146/annurev-criminol-051520-012342 https://doi.org/10.1146/annurev-criminol-051520-012342. 
